{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Title: Titanic Survival Prediction\n",
    "#### Done By: Nozipho Sithembiso Ndebele\n",
    "---\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://itsmeprasannak.wordpress.com/wp-content/uploads/2021/03/10-sn56-20201221-titanicsinking-hr.jpg?w=1440\" alt=\"Titanic Image\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#BC> Background Context</a>\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Data Collection and Description</a>\n",
    "\n",
    "<a href=#three>3. Loading Data </a>\n",
    "\n",
    "<a href=#four>4. Data Cleaning and Filtering</a>\n",
    "\n",
    "<a href=#five>5. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#six>6. Modeling </a>\n",
    "\n",
    "<a href=#seven>7. Evaluation and Validation</a>\n",
    "\n",
    "<a href=#eight>8. Final Model</a>\n",
    "\n",
    "<a href=#nine>9. Conclusion and Future Work</a>\n",
    "\n",
    "<a href=#ten>10. References</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    " <a id=\"BC\"></a>\n",
    "## **Background Context**\n",
    "\n",
    "### Purpose\n",
    "This project aims to analyze the Titanic passenger dataset to uncover patterns and factors that contributed to survival during the tragic sinking of the RMS Titanic. By applying data preprocessing, exploratory data analysis, and classification techniques, the project seeks to predict which types of passengers were more likely to survive.\n",
    "\n",
    "### Significance\n",
    "Understanding survival patterns on the Titanic is not only a classic problem in data science but also valuable for:\n",
    "\n",
    "* Identifying key demographic and socio-economic factors (e.g., gender, age, class) linked to survival\n",
    "\n",
    "* Highlighting potential biases in rescue decisions\n",
    "\n",
    "* Improving classification models by exploring real-world, historical datasets\n",
    "\n",
    "* Demonstrating the power of machine learning in uncovering insights from structured data\n",
    "\n",
    "This analysis helps build a strong foundation for applying data science skills to real-world problems, especially in binary classification and predictive modeling.\n",
    "\n",
    "### Problem Domain\n",
    "The Titanic dataset includes various features such as age, sex, passenger class, number of siblings/spouses aboard, and fare paid. The goal is to predict the target variable—Survived (0 = No, 1 = Yes)—based on these features. Developing a robust classification model involves understanding relationships between variables and identifying the most influential predictors of survival.\n",
    "### Challenges\n",
    "* Missing Data: Age and Cabin information contain missing values that must be handled.\n",
    "\n",
    "* Imbalanced Features: Some classes (e.g., 1st class vs. 3rd class) may have disproportionate survival rates.\n",
    "\n",
    "* Feature Engineering: Extracting meaningful insights from names, tickets, or titles (e.g., \"Mr.\", \"Mrs.\")\n",
    "\n",
    "* Model Evaluation: Selecting and comparing appropriate metrics like accuracy, precision, recall, and F1-score.\n",
    "\n",
    "### Key Questions\n",
    "* Which passenger features (e.g., gender, class, age) were most associated with survival?\n",
    "\n",
    "* How did socio-economic status affect the likelihood of survival?\n",
    "\n",
    "* What role did age and family relationships (siblings/parents onboard) play in survival outcomes?\n",
    "\n",
    "* Can we accurately predict a passenger’s survival using classification algorithms?\n",
    "\n",
    "* How do different models (e.g., logistic regression, decision trees, random forest) perform on this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#one></a>\n",
    "## **Importing Packages**\n",
    "\n",
    "### Purpose\n",
    "To set up the Python environment with the necessary libraries for data manipulation, visualization, and machine learning. These libraries will facilitate data preprocessing, feature extraction, model training, and evaluation.\n",
    "\n",
    "### Details\n",
    "* Pandas: For handling and analyzing data.\n",
    "\n",
    "* NumPy: For numerical operations.\n",
    "\n",
    "* Matplotlib/Seaborn: For data visualization to understand trends and patterns.\n",
    "\n",
    "* scikit-learn: For building and evaluating machine learning models.\n",
    "\n",
    "* NLTK/Spacy: For text preprocessing and natural language processing tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages  \n",
    "\n",
    "# Data manipulation and analysis  \n",
    "import pandas as pd  # Pandas for data handling  \n",
    "import numpy as np  # NumPy for numerical operations  \n",
    "\n",
    "# Data visualization  \n",
    "import matplotlib.pyplot as plt  # Matplotlib for static plots  \n",
    "import seaborn as sns  # Seaborn for statistical visualization  \n",
    "import plotly.express as px  # Plotly for interactive plots  \n",
    "\n",
    "# Natural Language Processing  \n",
    "import nltk  # Natural Language Toolkit  \n",
    "from nltk.corpus import stopwords  # Stopword removal  \n",
    "from nltk.tokenize import word_tokenize  # Tokenization  \n",
    "import re  # Regular expressions for text cleaning  \n",
    "\n",
    "# Configure visualization settings\n",
    "sns.set(style='whitegrid')  # Set the default style for Seaborn plots\n",
    "plt.rcParams['figure.figsize'] = (10, 6)  # Set default figure size for Matplotlib\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings  # Import the warnings module\n",
    "warnings.filterwarnings('ignore')  # Ignore all warning messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#two></a>\n",
    "## **Data Collection and Description**\n",
    "### Purpose\n",
    "This section provides an overview of the Titanic dataset and its structure. Understanding the dataset is essential to build accurate classification models and extract meaningful insights regarding survival patterns among passengers.\n",
    "\n",
    "### Details\n",
    "The dataset captures demographic and travel information of passengers aboard the RMS Titanic during its ill-fated maiden voyage. It includes attributes such as age, sex, passenger class, and fare paid, which can be used to predict survival outcomes.\n",
    "\n",
    "* Source: The dataset is publicly available on Kaggle as part of the Titanic Machine Learning competition.\n",
    "Link to dataset\n",
    "\n",
    "* Method of Collection: The dataset compiles historical records from the Titanic's passenger manifest, integrating demographic, ticketing, and survival information.\n",
    "\n",
    "* Size: The dataset contains 891 rows and 12 columns, where each row corresponds to an individual passenger.\n",
    "\n",
    "* Scope: The dataset includes information relevant to survival prediction, such as:\n",
    "\n",
    "  * Socio-economic status (ticket class and fare)\n",
    "\n",
    "  * Family relations aboard (siblings/spouses, parents/children)\n",
    "\n",
    "  * Passenger age and gender\n",
    "\n",
    "  * Embarkation port and cabin information\n",
    "\n",
    "* Types of Data:\n",
    "\n",
    "  * `Feature`\tDescription\n",
    "  * `PassengerId`\tUnique identifier for each passenger\n",
    "  * `Survived`\tSurvival status (0 = No, 1 = Yes)\n",
    "  * `Pclass`\tPassenger class (1 = Upper, 2 = Middle, 3 = Lower)\n",
    "  * `Name`\tFull name, including title\n",
    "  * `Sex`\tGender of the passenger\n",
    "  * `Age`\tAge in years (some missing values)\n",
    "  * `SibSp`\tNumber of siblings/spouses aboard\n",
    "  *` Parch`\tNumber of parents/children aboard\n",
    "  * `Ticket`\tTicket number\n",
    "  * `Fare`\tFare paid for the ticket\n",
    "  * `Cabin`\tCabin number (many missing values)\n",
    "  * `Embarked`\tPort of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#three></a>\n",
    "## **Loading Data**\n",
    "### Purpose\n",
    "The purpose of this section is to load the dataset into the notebook for further manipulation and analysis. This is the first step in working with the data, and it allows us to inspect the raw data and get a sense of its structure.\n",
    "\n",
    "### Details\n",
    "In this section, we will load the dataset into a Pandas DataFrame and display the first few rows to understand what the raw data looks like. This will help in planning the next steps of data cleaning and analysis.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a Pandas DataFrame\n",
    "\n",
    "# The dataset is stored in a CSV file named 'Domestic violence.csv'\n",
    "df = pd.read_csv('Titanic-Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df is the original dataset (DataFrame), this creates a copy of it\n",
    "df_copy = df.copy()\n",
    "\n",
    "# Now 'df_copy' is an independent copy of 'df'. Changes to 'df_copy' won't affect 'df'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the dataset to get a sense of what the raw data looks like\n",
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the number of rows and columns in the dataset to understand its size\n",
    "df_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Check the structure of the dataset\n",
    "df_copy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#four></a>\n",
    "## **Data Cleaning and Filtering**\n",
    "Before analyzing the data, it is crucial to clean and filter it. This process involves handling missing values, removing outliers, correcting errors, and possibly reducing the data by filtering out irrelevant features. These steps ensure that the analysis is based on accurate and reliable data.\n",
    "\n",
    "Details\n",
    "In this section, we will:\n",
    "\n",
    "* Check for Missing Values: Identify if there are any missing values in the dataset and handle them accordingly.\n",
    "* Remove Duplicates: Ensure there are no duplicate rows that could skew the analysis.\n",
    "* Correct Errors: Look for and correct any obvious data entry errors.\n",
    "* Filter Data: Depending on the analysis requirements, filter the data to include only relevant records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check for missing values in the dataset\n",
    "\n",
    "def check_missing_values(df):\n",
    "    \"\"\"\n",
    "    Check for missing values in the dataset and display the number of missing values per column.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataset to check for missing values.\n",
    "\n",
    "    Returns:\n",
    "    pandas.Series: A series showing the number of missing values for each column.\n",
    "    \"\"\"\n",
    "     # Check for missing values in the dataset and display them\n",
    "    print(\"Missing values per column:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(missing_values)\n",
    "    return missing_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "missing_values = check_missing_values(df_copy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After examining the dataset, missing values were identified in some columns. All other columns are complete, ensuring data integrity for most features. Depending on the analysis goals, handling these missing values may be necessary through imputation or other data-cleaning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_missing_values(df):\n",
    "    \"\"\"\n",
    "    Check for missing values in the dataset, display the number of missing values per column,\n",
    "    handle them appropriately, and drop the 'Cabin' column.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataset to check and clean missing values.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The cleaned dataset with missing values handled.\n",
    "    \"\"\"\n",
    "    print(\"Missing values per column:\\n\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    \n",
    "    for column, count in missing_values.items():\n",
    "        if count > 0:\n",
    "            print(f\"{column}: {count} missing\")\n",
    "            if column == 'Age':\n",
    "                # Fill 'Age' with the median\n",
    "                median_age = df['Age'].median()\n",
    "                df['Age'].fillna(median_age, inplace=True)\n",
    "                print(f\"  ➤ Filled 'Age' with median: {median_age}\")\n",
    "            elif column == 'Cabin':\n",
    "                # Drop the 'Cabin' column\n",
    "                df.drop('Cabin', axis=1, inplace=True)\n",
    "                print(\"  ➤ Dropped 'Cabin' column.\")\n",
    "            elif column == 'Embarked':\n",
    "                # Fill 'Embarked' with the mode\n",
    "                mode_embarked = df['Embarked'].mode()[0]\n",
    "                df['Embarked'].fillna(mode_embarked, inplace=True)\n",
    "                print(f\"  ➤ Filled 'Embarked' with mode: {mode_embarked}\")\n",
    "            else:\n",
    "                print(\"  ➤ No specific rule. Consider domain-specific handling.\")\n",
    "        else:\n",
    "            print(f\"{column}: No missing values\")\n",
    "    \n",
    "    print(\"\\nMissing values handled.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "\n",
      "PassengerId: No missing values\n",
      "Survived: No missing values\n",
      "Pclass: No missing values\n",
      "Name: No missing values\n",
      "Sex: No missing values\n",
      "Age: 177 missing\n",
      "  ➤ Filled 'Age' with median: 28.0\n",
      "SibSp: No missing values\n",
      "Parch: No missing values\n",
      "Ticket: No missing values\n",
      "Fare: No missing values\n",
      "Cabin: 687 missing\n",
      "  ➤ Dropped 'Cabin' column.\n",
      "Embarked: 2 missing\n",
      "  ➤ Filled 'Embarked' with mode: S\n",
      "\n",
      "Missing values handled.\n"
     ]
    }
   ],
   "source": [
    "# Call the function\n",
    "df_copy = handling_missing_values(df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df):\n",
    "    \"\"\"\n",
    "    Checks for duplicate rows in the dataset and removes them if any are found.\n",
    "\n",
    "    Args:\n",
    "    df (pandas.DataFrame): The dataframe to check for duplicate rows.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The dataframe with duplicate rows removed, if any existed.\n",
    "    \"\"\"\n",
    "    # Check for duplicate rows\n",
    "    duplicate_rows = df.duplicated().sum()\n",
    "    print(f\"\\nNumber of duplicate rows: {duplicate_rows}\")\n",
    "    \n",
    "    # Remove duplicates if any exist\n",
    "    if duplicate_rows > 0:\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        print(f\"Duplicate rows removed. Updated dataframe has {len(df)} rows.\")\n",
    "    else:\n",
    "        print(\"No duplicate rows found.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of duplicate rows: 0\n",
      "No duplicate rows found.\n"
     ]
    }
   ],
   "source": [
    "df_copy = remove_duplicates(df_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon reviewing the dataset, no duplicate rows were found. This ensures that all records are unique, and no further action is required for data deduplication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Saving the Cleaned Dataset**\n",
    "### Purpose\n",
    "\n",
    "This section outlines how to save the cleaned dataset for future use. Saving the dataset ensures that the data cleaning process does not need to be repeated and allows for consistent use in subsequent analyses.\n",
    "\n",
    "### Details\n",
    "\n",
    "We will save the cleaned dataset as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Save the cleaned dataset to a new CSV file\n",
    "\n",
    "def save_cleaned_dataset(df, filename='Titanic_Survival_cleaned.csv'):\n",
    "    \"\"\"\n",
    "    Saves the cleaned dataframe to a CSV file.\n",
    "\n",
    "    Args:\n",
    "    df (pandas.DataFrame): The cleaned dataframe to save.\n",
    "    filename (str): The name of the file to save the dataframe to (default is 'cleaned_domestic_violence.csv').\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Save the cleaned dataset to a CSV file\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Cleaned dataset saved successfully as '{filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved successfully as 'Titanic_Survival_cleaned.csv'.\n"
     ]
    }
   ],
   "source": [
    "save_cleaned_dataset(df_copy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#five></a>\n",
    "## **Exploratory Data Analysis (EDA)**\n",
    "\n",
    "It is the process of analyzing datasets to summarize key features, often through visualization methods. It aims to discover patterns, spot anomalies, and formulate hypotheses for deeper insights, which informs subsequent analysis.\n",
    "#### Advantages\n",
    "\n",
    "- Helps in understanding the data before modeling.\n",
    "- Provides insights that guide feature selection and engineering.\n",
    "- Assists in choosing appropriate modeling techniques.\n",
    "- Uncovers potential data quality issues early.\n",
    "\n",
    "`The following methods were employed to communicate our objective:`\n",
    "\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#nine></a>\n",
    "## **Conclusion and Future Work**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion\n",
    "\n",
    "\n",
    "\n",
    "##### Future Work\n",
    "\n",
    "To build upon this study, future work could focus on the following areas:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#ten></a>\n",
    "## **References**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Sections to Consider\n",
    "\n",
    "**Contributors**: Nozipho Sithembiso Ndebele\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
